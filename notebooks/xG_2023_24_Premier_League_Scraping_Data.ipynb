{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2BEpiXuQbUI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "f636fd8d-906f-4b84-fcf4-0b7d12936431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.3.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.1)\n",
            "Starting to scrape matches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Matches:  34%|███▍      | 130/380 [02:22<04:29,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match 22055 not found (status code 404).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Matches: 100%|██████████| 380/380 [06:55<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ CSV file successfully saved at: /content/EPL_2023_24_all_matches_shots.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8e78e343-57ae-4631-badf-e773af6406a6\", \"EPL_2023_24_all_matches_shots.csv\", 1987045)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install dependencies (if not already installed)\n",
        "!pip install lxml beautifulsoup4 tqdm\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "# --- Utility Functions ---\n",
        "\n",
        "def scrape_match(match_id):\n",
        "    \"\"\"\n",
        "    Scrapes shot data from a single match on Understat.\n",
        "    Returns a DataFrame if successful, or None if not.\n",
        "    \"\"\"\n",
        "    url = f\"https://understat.com/match/{match_id}\"\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        # Skip if the page isn't found\n",
        "        if res.status_code != 200:\n",
        "            print(f\"Match {match_id} not found (status code {res.status_code}).\")\n",
        "            return None\n",
        "\n",
        "        soup = BeautifulSoup(res.content, 'lxml')\n",
        "        scripts = soup.find_all('script')\n",
        "        # Check if the script that contains JSON data is available\n",
        "        if len(scripts) < 2 or scripts[1].string is None:\n",
        "            print(f\"Match {match_id}: Could not find the proper script element.\")\n",
        "            return None\n",
        "\n",
        "        strings = scripts[1].string\n",
        "        # Extract the JSON data using the known pattern\n",
        "        ind_start = strings.index(\"('\") + 2\n",
        "        ind_end = strings.index(\"')\")\n",
        "        json_data = strings[ind_start:ind_end]\n",
        "        json_data = json_data.encode('utf8').decode('unicode_escape')\n",
        "        data = json.loads(json_data)\n",
        "\n",
        "        # Create DataFrames for home and away shot data and add a match_id column\n",
        "        df_h = pd.DataFrame(data.get('h', []))\n",
        "        df_a = pd.DataFrame(data.get('a', []))\n",
        "        if df_h.empty and df_a.empty:\n",
        "            return None\n",
        "\n",
        "        df = pd.concat([df_h, df_a], ignore_index=True)\n",
        "        df['match_id'] = match_id\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing match {match_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Main Loop for Season Scraping ---\n",
        "\n",
        "# Set the range of match IDs to scrape.\n",
        "# Adjust these numbers if needed for the specific season.\n",
        "start_id = 21925\n",
        "end_id = 22305  # This will loop through match IDs 22200 to 22299\n",
        "\n",
        "all_matches = []  # List to store DataFrames from each match\n",
        "\n",
        "print(\"Starting to scrape matches...\")\n",
        "for match_id in tqdm(range(start_id, end_id), desc=\"Processing Matches\"):\n",
        "    df_match = scrape_match(match_id)\n",
        "    if df_match is not None:\n",
        "        all_matches.append(df_match)\n",
        "    time.sleep(0.5)  # Optional: slow down requests to be polite to the server\n",
        "\n",
        "# Combine all the match DataFrames if any data was collected\n",
        "if all_matches:\n",
        "    final_df = pd.concat(all_matches, ignore_index=True)\n",
        "    output_path = '/content/EPL_2023_24_all_matches_shots.csv'\n",
        "    final_df.to_csv(output_path, index=False)\n",
        "    print(f\"\\n✅ CSV file successfully saved at: {output_path}\")\n",
        "    # Auto-download the file to your local machine\n",
        "    files.download(output_path)\n",
        "else:\n",
        "    print(\"⚠️ No match data was successfully scraped.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Scraping current season's data:\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Step 1: Upload your existing CSV to Colab first manually or with files.upload()\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Replace with the name of your uploaded file\n",
        "existing_file = \"EPL_2023-24_all_matches_shots.csv\"\n",
        "\n",
        "# Step 2: Load existing data\n",
        "try:\n",
        "    df_existing = pd.read_csv(existing_file)\n",
        "    print(f\"✅ Existing data loaded: {df_existing.shape[0]} rows\")\n",
        "except FileNotFoundError:\n",
        "    df_existing = pd.DataFrame()\n",
        "    print(\"⚠️ Existing file not found. Starting with empty DataFrame.\")\n",
        "\n",
        "# Step 3: Define scraper for individual match\n",
        "def scrape_match(match_id):\n",
        "    try:\n",
        "        url = f\"https://understat.com/match/{match_id}\"\n",
        "        res = requests.get(url)\n",
        "        soup = BeautifulSoup(res.content, 'lxml')\n",
        "        scripts = soup.find_all('script')\n",
        "        strings = scripts[1].string\n",
        "        ind_start = strings.index(\"('\") + 2\n",
        "        ind_end = strings.index(\"')\")\n",
        "        json_data = strings[ind_start:ind_end]\n",
        "        json_data = json_data.encode('utf8').decode('unicode_escape')\n",
        "        data = json.loads(json_data)\n",
        "        df_h = pd.DataFrame(data['h'])\n",
        "        df_a = pd.DataFrame(data['a'])\n",
        "        df = pd.concat([df_h, df_a], ignore_index=True)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing match {match_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 4: Scrape new matches (26602–26908)\n",
        "new_matches = []\n",
        "\n",
        "for match_id in range(26602, 26909):\n",
        "    print(f\"🔍 Scraping match {match_id}...\")\n",
        "    df_match = scrape_match(match_id)\n",
        "    if df_match is not None and not df_match.empty:\n",
        "        new_matches.append(df_match)\n",
        "\n",
        "# Step 5: Combine new data\n",
        "if new_matches:\n",
        "    df_new = pd.concat(new_matches, ignore_index=True)\n",
        "    print(f\"✅ New matches scraped: {df_new.shape[0]} rows\")\n",
        "else:\n",
        "    print(\"⚠️ No new match data found.\")\n",
        "    df_new = pd.DataFrame()\n",
        "\n",
        "# Step 6: Append new data to existing\n",
        "if not df_new.empty:\n",
        "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "    output_file = \"EPL_2023-24_all_matches_shots_UPDATED.csv\"\n",
        "    df_combined.to_csv(output_file, index=False)\n",
        "    print(f\"✅ Combined data saved! Total rows: {df_combined.shape[0]}\")\n",
        "\n",
        "    # Step 7: Download the updated file to your local machine\n",
        "    files.download(output_file)\n",
        "else:\n",
        "    print(\"⚠️ No new data to append. Existing file remains unchanged.\")\n"
      ],
      "metadata": {
        "id": "B9Om4bwKOA1Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32573e94-0d6a-46be-a02a-f9bbc46987f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b544ccb1-d64f-4a07-ab8f-0ba907198d82\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b544ccb1-d64f-4a07-ab8f-0ba907198d82\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving EPL_2023-24_all_matches_shots.csv to EPL_2023-24_all_matches_shots.csv\n",
            "✅ Existing data loaded: 10492 rows\n",
            "🔍 Scraping match 26602...\n",
            "🔍 Scraping match 26603...\n",
            "🔍 Scraping match 26604...\n",
            "🔍 Scraping match 26605...\n",
            "🔍 Scraping match 26606...\n",
            "🔍 Scraping match 26607...\n",
            "🔍 Scraping match 26608...\n",
            "🔍 Scraping match 26609...\n",
            "🔍 Scraping match 26610...\n",
            "🔍 Scraping match 26611...\n",
            "🔍 Scraping match 26612...\n",
            "🔍 Scraping match 26613...\n",
            "🔍 Scraping match 26614...\n",
            "🔍 Scraping match 26615...\n",
            "🔍 Scraping match 26616...\n",
            "🔍 Scraping match 26617...\n",
            "🔍 Scraping match 26618...\n",
            "🔍 Scraping match 26619...\n",
            "🔍 Scraping match 26620...\n",
            "🔍 Scraping match 26621...\n",
            "🔍 Scraping match 26622...\n",
            "🔍 Scraping match 26623...\n",
            "🔍 Scraping match 26624...\n",
            "🔍 Scraping match 26625...\n",
            "🔍 Scraping match 26626...\n",
            "🔍 Scraping match 26627...\n",
            "🔍 Scraping match 26628...\n",
            "🔍 Scraping match 26629...\n",
            "🔍 Scraping match 26630...\n",
            "🔍 Scraping match 26631...\n",
            "🔍 Scraping match 26632...\n",
            "🔍 Scraping match 26633...\n",
            "🔍 Scraping match 26634...\n",
            "🔍 Scraping match 26635...\n",
            "🔍 Scraping match 26636...\n",
            "🔍 Scraping match 26637...\n",
            "🔍 Scraping match 26638...\n",
            "🔍 Scraping match 26639...\n",
            "🔍 Scraping match 26640...\n",
            "🔍 Scraping match 26641...\n",
            "🔍 Scraping match 26642...\n",
            "🔍 Scraping match 26643...\n",
            "🔍 Scraping match 26644...\n",
            "🔍 Scraping match 26645...\n",
            "🔍 Scraping match 26646...\n",
            "🔍 Scraping match 26647...\n",
            "🔍 Scraping match 26648...\n",
            "🔍 Scraping match 26649...\n",
            "🔍 Scraping match 26650...\n",
            "🔍 Scraping match 26651...\n",
            "🔍 Scraping match 26652...\n",
            "🔍 Scraping match 26653...\n",
            "🔍 Scraping match 26654...\n",
            "🔍 Scraping match 26655...\n",
            "🔍 Scraping match 26656...\n",
            "🔍 Scraping match 26657...\n",
            "🔍 Scraping match 26658...\n",
            "🔍 Scraping match 26659...\n",
            "🔍 Scraping match 26660...\n",
            "🔍 Scraping match 26661...\n",
            "🔍 Scraping match 26662...\n",
            "🔍 Scraping match 26663...\n",
            "🔍 Scraping match 26664...\n",
            "🔍 Scraping match 26665...\n",
            "🔍 Scraping match 26666...\n",
            "🔍 Scraping match 26667...\n",
            "🔍 Scraping match 26668...\n",
            "🔍 Scraping match 26669...\n",
            "🔍 Scraping match 26670...\n",
            "🔍 Scraping match 26671...\n",
            "🔍 Scraping match 26672...\n",
            "🔍 Scraping match 26673...\n",
            "🔍 Scraping match 26674...\n",
            "🔍 Scraping match 26675...\n",
            "🔍 Scraping match 26676...\n",
            "🔍 Scraping match 26677...\n",
            "🔍 Scraping match 26678...\n",
            "🔍 Scraping match 26679...\n",
            "🔍 Scraping match 26680...\n",
            "🔍 Scraping match 26681...\n",
            "🔍 Scraping match 26682...\n",
            "🔍 Scraping match 26683...\n",
            "🔍 Scraping match 26684...\n",
            "🔍 Scraping match 26685...\n",
            "🔍 Scraping match 26686...\n",
            "🔍 Scraping match 26687...\n",
            "🔍 Scraping match 26688...\n",
            "🔍 Scraping match 26689...\n",
            "🔍 Scraping match 26690...\n",
            "🔍 Scraping match 26691...\n",
            "🔍 Scraping match 26692...\n",
            "🔍 Scraping match 26693...\n",
            "🔍 Scraping match 26694...\n",
            "🔍 Scraping match 26695...\n",
            "🔍 Scraping match 26696...\n",
            "🔍 Scraping match 26697...\n",
            "🔍 Scraping match 26698...\n",
            "🔍 Scraping match 26699...\n",
            "🔍 Scraping match 26700...\n",
            "🔍 Scraping match 26701...\n",
            "🔍 Scraping match 26702...\n",
            "🔍 Scraping match 26703...\n",
            "🔍 Scraping match 26704...\n",
            "🔍 Scraping match 26705...\n",
            "🔍 Scraping match 26706...\n",
            "🔍 Scraping match 26707...\n",
            "🔍 Scraping match 26708...\n",
            "🔍 Scraping match 26709...\n",
            "🔍 Scraping match 26710...\n",
            "🔍 Scraping match 26711...\n",
            "🔍 Scraping match 26712...\n",
            "🔍 Scraping match 26713...\n",
            "🔍 Scraping match 26714...\n",
            "🔍 Scraping match 26715...\n",
            "🔍 Scraping match 26716...\n",
            "🔍 Scraping match 26717...\n",
            "🔍 Scraping match 26718...\n",
            "🔍 Scraping match 26719...\n",
            "🔍 Scraping match 26720...\n",
            "🔍 Scraping match 26721...\n",
            "🔍 Scraping match 26722...\n",
            "🔍 Scraping match 26723...\n",
            "🔍 Scraping match 26724...\n",
            "🔍 Scraping match 26725...\n",
            "🔍 Scraping match 26726...\n",
            "🔍 Scraping match 26727...\n",
            "🔍 Scraping match 26728...\n",
            "🔍 Scraping match 26729...\n",
            "🔍 Scraping match 26730...\n",
            "🔍 Scraping match 26731...\n",
            "🔍 Scraping match 26732...\n",
            "🔍 Scraping match 26733...\n",
            "🔍 Scraping match 26734...\n",
            "🔍 Scraping match 26735...\n",
            "🔍 Scraping match 26736...\n",
            "🔍 Scraping match 26737...\n",
            "🔍 Scraping match 26738...\n",
            "🔍 Scraping match 26739...\n",
            "🔍 Scraping match 26740...\n",
            "🔍 Scraping match 26741...\n",
            "🔍 Scraping match 26742...\n",
            "🔍 Scraping match 26743...\n",
            "🔍 Scraping match 26744...\n",
            "🔍 Scraping match 26745...\n",
            "🔍 Scraping match 26746...\n",
            "🔍 Scraping match 26747...\n",
            "🔍 Scraping match 26748...\n",
            "🔍 Scraping match 26749...\n",
            "🔍 Scraping match 26750...\n",
            "🔍 Scraping match 26751...\n",
            "🔍 Scraping match 26752...\n",
            "🔍 Scraping match 26753...\n",
            "🔍 Scraping match 26754...\n",
            "🔍 Scraping match 26755...\n",
            "🔍 Scraping match 26756...\n",
            "🔍 Scraping match 26757...\n",
            "🔍 Scraping match 26758...\n",
            "🔍 Scraping match 26759...\n",
            "🔍 Scraping match 26760...\n",
            "🔍 Scraping match 26761...\n",
            "🔍 Scraping match 26762...\n",
            "🔍 Scraping match 26763...\n",
            "🔍 Scraping match 26764...\n",
            "🔍 Scraping match 26765...\n",
            "🔍 Scraping match 26766...\n",
            "🔍 Scraping match 26767...\n",
            "🔍 Scraping match 26768...\n",
            "🔍 Scraping match 26769...\n",
            "🔍 Scraping match 26770...\n",
            "🔍 Scraping match 26771...\n",
            "🔍 Scraping match 26772...\n",
            "🔍 Scraping match 26773...\n",
            "🔍 Scraping match 26774...\n",
            "🔍 Scraping match 26775...\n",
            "🔍 Scraping match 26776...\n",
            "🔍 Scraping match 26777...\n",
            "🔍 Scraping match 26778...\n",
            "🔍 Scraping match 26779...\n",
            "🔍 Scraping match 26780...\n",
            "🔍 Scraping match 26781...\n",
            "🔍 Scraping match 26782...\n",
            "🔍 Scraping match 26783...\n",
            "🔍 Scraping match 26784...\n",
            "🔍 Scraping match 26785...\n",
            "🔍 Scraping match 26786...\n",
            "🔍 Scraping match 26787...\n",
            "🔍 Scraping match 26788...\n",
            "🔍 Scraping match 26789...\n",
            "🔍 Scraping match 26790...\n",
            "🔍 Scraping match 26791...\n",
            "🔍 Scraping match 26792...\n",
            "🔍 Scraping match 26793...\n",
            "🔍 Scraping match 26794...\n",
            "🔍 Scraping match 26795...\n",
            "🔍 Scraping match 26796...\n",
            "🔍 Scraping match 26797...\n",
            "🔍 Scraping match 26798...\n",
            "🔍 Scraping match 26799...\n",
            "🔍 Scraping match 26800...\n",
            "🔍 Scraping match 26801...\n",
            "🔍 Scraping match 26802...\n",
            "🔍 Scraping match 26803...\n",
            "🔍 Scraping match 26804...\n",
            "🔍 Scraping match 26805...\n",
            "🔍 Scraping match 26806...\n",
            "🔍 Scraping match 26807...\n",
            "🔍 Scraping match 26808...\n",
            "🔍 Scraping match 26809...\n",
            "🔍 Scraping match 26810...\n",
            "🔍 Scraping match 26811...\n",
            "🔍 Scraping match 26812...\n",
            "🔍 Scraping match 26813...\n",
            "🔍 Scraping match 26814...\n",
            "🔍 Scraping match 26815...\n",
            "🔍 Scraping match 26816...\n",
            "🔍 Scraping match 26817...\n",
            "🔍 Scraping match 26818...\n",
            "🔍 Scraping match 26819...\n",
            "🔍 Scraping match 26820...\n",
            "🔍 Scraping match 26821...\n",
            "🔍 Scraping match 26822...\n",
            "🔍 Scraping match 26823...\n",
            "🔍 Scraping match 26824...\n",
            "🔍 Scraping match 26825...\n",
            "🔍 Scraping match 26826...\n",
            "🔍 Scraping match 26827...\n",
            "🔍 Scraping match 26828...\n",
            "🔍 Scraping match 26829...\n",
            "🔍 Scraping match 26830...\n",
            "🔍 Scraping match 26831...\n",
            "🔍 Scraping match 26832...\n",
            "🔍 Scraping match 26833...\n",
            "🔍 Scraping match 26834...\n",
            "🔍 Scraping match 26835...\n",
            "🔍 Scraping match 26836...\n",
            "🔍 Scraping match 26837...\n",
            "🔍 Scraping match 26838...\n",
            "🔍 Scraping match 26839...\n",
            "🔍 Scraping match 26840...\n",
            "🔍 Scraping match 26841...\n",
            "🔍 Scraping match 26842...\n",
            "🔍 Scraping match 26843...\n",
            "🔍 Scraping match 26844...\n",
            "🔍 Scraping match 26845...\n",
            "🔍 Scraping match 26846...\n",
            "🔍 Scraping match 26847...\n",
            "🔍 Scraping match 26848...\n",
            "🔍 Scraping match 26849...\n",
            "🔍 Scraping match 26850...\n",
            "🔍 Scraping match 26851...\n",
            "🔍 Scraping match 26852...\n",
            "🔍 Scraping match 26853...\n",
            "🔍 Scraping match 26854...\n",
            "🔍 Scraping match 26855...\n",
            "🔍 Scraping match 26856...\n",
            "🔍 Scraping match 26857...\n",
            "🔍 Scraping match 26858...\n",
            "🔍 Scraping match 26859...\n",
            "🔍 Scraping match 26860...\n",
            "🔍 Scraping match 26861...\n",
            "🔍 Scraping match 26862...\n",
            "🔍 Scraping match 26863...\n",
            "🔍 Scraping match 26864...\n",
            "🔍 Scraping match 26865...\n",
            "🔍 Scraping match 26866...\n",
            "🔍 Scraping match 26867...\n",
            "🔍 Scraping match 26868...\n",
            "🔍 Scraping match 26869...\n",
            "🔍 Scraping match 26870...\n",
            "🔍 Scraping match 26871...\n",
            "🔍 Scraping match 26872...\n",
            "🔍 Scraping match 26873...\n",
            "🔍 Scraping match 26874...\n",
            "🔍 Scraping match 26875...\n",
            "🔍 Scraping match 26876...\n",
            "🔍 Scraping match 26877...\n",
            "🔍 Scraping match 26878...\n",
            "🔍 Scraping match 26879...\n",
            "🔍 Scraping match 26880...\n",
            "🔍 Scraping match 26881...\n",
            "🔍 Scraping match 26882...\n",
            "🔍 Scraping match 26883...\n",
            "🔍 Scraping match 26884...\n",
            "🔍 Scraping match 26885...\n",
            "🔍 Scraping match 26886...\n",
            "🔍 Scraping match 26887...\n",
            "🔍 Scraping match 26888...\n",
            "🔍 Scraping match 26889...\n",
            "🔍 Scraping match 26890...\n",
            "⚠️ Error processing match 26890: list index out of range\n",
            "🔍 Scraping match 26891...\n",
            "🔍 Scraping match 26892...\n",
            "🔍 Scraping match 26893...\n",
            "🔍 Scraping match 26894...\n",
            "🔍 Scraping match 26895...\n",
            "🔍 Scraping match 26896...\n",
            "🔍 Scraping match 26897...\n",
            "🔍 Scraping match 26898...\n",
            "🔍 Scraping match 26899...\n",
            "🔍 Scraping match 26900...\n",
            "🔍 Scraping match 26901...\n",
            "🔍 Scraping match 26902...\n",
            "🔍 Scraping match 26903...\n",
            "🔍 Scraping match 26904...\n",
            "🔍 Scraping match 26905...\n",
            "🔍 Scraping match 26906...\n",
            "🔍 Scraping match 26907...\n",
            "🔍 Scraping match 26908...\n",
            "✅ New matches scraped: 7979 rows\n",
            "✅ Combined data saved! Total rows: 18471\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bc8aaaad-7f93-461d-86e2-d0c678b5df11\", \"EPL_2023-24_all_matches_shots_UPDATED.csv\", 3206862)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6WsY_b1qAFQS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}